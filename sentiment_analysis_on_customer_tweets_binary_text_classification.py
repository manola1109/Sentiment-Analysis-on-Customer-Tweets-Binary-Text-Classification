# -*- coding: utf-8 -*-
"""Sentiment-Analysis-on-Customer-Tweets-Binary-Text-Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QU3X_WAl2rnyhn3u83vqHd-hju0lKaYI
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from collections import Counter
import itertools
import nltk
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional

nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')

print("Train shape:", train.shape)
print("Test shape:", test.shape)
print("\nTrain sample:\n", train.head())
print("\nMissing values in train:\n", train.isnull().sum())
print("\nMissing values in test:\n", test.isnull().sum())

# Label distribution
plt.figure(figsize=(6,4))
sns.countplot(x='label', data=train)
plt.title('Label Distribution')
plt.show()

# Tweet length stats
train['tweet_len'] = train['tweet'].apply(lambda x: len(str(x).split()))
print("\nTweet length statistics:\n", train['tweet_len'].describe())
plt.figure(figsize=(6,4))
sns.histplot(train['tweet_len'], bins=30)
plt.title('Tweet Length Distribution')
plt.show()

# Most common words (before cleaning)
all_words = list(itertools.chain.from_iterable(
    [str(t).lower().split() for t in train['tweet']]))
common_words = Counter(all_words).most_common(20)
print("\nMost common words (raw):\n", common_words)

stop_words = set(stopwords.words('english'))
ps = PorterStemmer()

def advanced_clean(text):
    text = str(text).lower()
    text = re.sub(r"http\S+", "", text)            # remove urls
    text = re.sub(r"@\w+", "", text)               # remove mentions
    text = re.sub(r"#\w+", "", text)               # remove hashtags
    text = re.sub(r"[^a-zA-Z\s]", "", text)        # remove punctuation
    text = re.sub(r"\s+", " ", text).strip()
    tokens = text.split()
    tokens = [w for w in tokens if w not in stop_words]  # remove stopwords
    tokens = [ps.stem(w) for w in tokens]                # stemming
    return " ".join(tokens)

train['clean_tweet'] = train['tweet'].apply(advanced_clean)
test['clean_tweet'] = test['tweet'].apply(advanced_clean)

# Show original vs cleaned tweet
print("\nOriginal Tweet:\n", train['tweet'].iloc[0])
print("Cleaned Tweet:\n", train['clean_tweet'].iloc[0])

# Most common words after cleaning
all_words_clean = list(itertools.chain.from_iterable(
    [t.split() for t in train['clean_tweet']]))
common_words_clean = Counter(all_words_clean).most_common(20)
print("\nMost common words (cleaned):\n", common_words_clean)

#TOKENIZATION & PADDING
max_words = 10000
max_len = 40

tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(train['clean_tweet'])

X = tokenizer.texts_to_sequences(train['clean_tweet'])
X = pad_sequences(X, maxlen=max_len, padding='post', truncating='post')
y = train['label']

X_test = tokenizer.texts_to_sequences(test['clean_tweet'])
X_test = pad_sequences(X_test, maxlen=max_len, padding='post', truncating='post')

# Split train/validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)

model = Sequential([
    Embedding(input_dim=max_words, output_dim=64, input_shape=(max_len,)),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.5),
    Bidirectional(LSTM(32)),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.build(input_shape=(None, max_len))  # Ensure model is built for summary
model.summary()

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=5,  # Increase for better results
    batch_size=32,
    verbose=1
)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.tight_layout()
plt.show()

word_index = tokenizer.word_index
reverse_word_index = {v:k for k,v in word_index.items()}
def decode_review(text):
    return " ".join([reverse_word_index.get(i, "?") for i in text if i != 0])

print("\nDecoded example input:\n", decode_review(X_train[0]))

test_preds = model.predict(X_test)
test_preds = (test_preds > 0.5).astype(int).reshape(-1)

submission = pd.DataFrame({'id': test['id'], 'label': test_preds})
submission.to_csv('submission.csv', index=False)
print("\nSubmission file created: submission.csv")